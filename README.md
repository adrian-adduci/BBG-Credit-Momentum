# BBG-Credit-Momentum

**Machine Learning Decision Support System for Bloomberg Credit Analytics**

A Streamlit application that analyzes economic data to identify momentum drivers for credit trading using XGBoost and sklearn models.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Table of Contents

- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Data Sources](#data-sources)
- [Model Details](#model-details)
- [Project Structure](#project-structure)
- [Recent Improvements](#recent-improvements)
- [Contributing](#contributing)
- [License](#license)

---

## Features

### Core Functionality
- **Interactive Streamlit Dashboard** - Real-time model training and visualization
- **XGBoost Time Series Forecasting** - Predict credit spreads 1-30 days ahead
- **Momentum Feature Engineering** - Automated rolling average calculations
- **Feature Importance Analysis** - Identify key momentum drivers over time
- **Predictive Power Scoring** - Rank features by predictive capability
- **Model Performance Metrics** - MAE, MSE, RMSE with visualizations

### Data Sources
- **Excel Files** (Bloomberg exports) - Fully supported
- **CSV Files** - Fully supported
- **Bloomberg API** - Template provided (requires completion)
- **Extensible Architecture** - Easy to add custom data sources

### Configuration
- **YAML Configuration** - Centralized settings management
- **Environment Variables** - Secure credential storage
- **Flexible Parameters** - Customize models, features, and analysis

---

## Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/adrian-adduci/BBG-Credit-Momentum.git
cd BBG-Credit-Momentum

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the application
streamlit run webapp.py
```

The dashboard will open in your browser at `http://localhost:8501`.

---

## Installation

### Prerequisites

- **Python 3.8+** (tested on 3.8, 3.9, 3.10, 3.11)
- **pip** package manager
- **Bloomberg Terminal** (optional, for API integration)

### Step 1: Clone Repository

```bash
git clone https://github.com/adrian-adduci/BBG-Credit-Momentum.git
cd BBG-Credit-Momentum
```

### Step 2: Create Virtual Environment (Recommended)

#### Windows:
```bash
python -m venv venv
venv\Scripts\activate
```

#### macOS/Linux:
```bash
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies installed** (all updated to latest stable versions):
- `streamlit==1.39.0` - Web interface
- `pandas==2.2.3` - Data manipulation
- `numpy==1.26.4` - Numerical computing
- `scikit-learn==1.5.2` - ML algorithms
- `xgboost==2.1.2` - Gradient boosting
- `matplotlib==3.9.2` - Plotting
- `seaborn==0.13.2` - Statistical visualizations
- `plotly==5.24.1` - Interactive charts
- `ppscore==1.3.0` - Predictive power scoring
- Plus 6 more (see [requirements.txt](requirements.txt))

### Step 4: Configure Application (Optional)

```bash
# Copy configuration templates
cp config.example.yaml config.yaml
cp .env.example .env

# Edit with your settings
nano config.yaml  # or use your preferred editor
nano .env
```

---

## Usage

### Method 1: Web Interface (Recommended)

1. **Start the application:**
   ```bash
   streamlit run webapp.py
   ```

2. **Upload data:**
   - Click "Browse files" in the sidebar
   - Select your Excel file (Bloomberg export format)
   - Example file provided: `data/Economic_Data_2020_08_01.xlsx`

3. **Configure features:**
   - **Target Feature**: Column to predict (e.g., `LF98TRUU_Index_OAS`)
   - **Momentum Parameters**: Columns for rolling averages (comma-separated)
   - **Date Range**: Filter data to specific time period

4. **Load and train:**
   - Click "Load Data" to preprocess
   - Select model (XGBoost recommended)
   - Click "Train Model" to start training

5. **View results:**
   - **Historic Data**: Visualize trends over time
   - **Feature Importance & Model Analysis**:
     - Model metrics (MAE, MSE, RMSE)
     - Forecast vs actual comparison
     - Feature importance rankings
     - Predictive power scores

### Method 2: Python API

```python
from _preprocessing import _preprocess_xlsx
from _models import _build_model

# Load and preprocess data
pipeline = _preprocess_xlsx(
    xlsx_file="data/Economic_Data_2020_08_01.xlsx",
    target_col="LF98TRUU_Index_OAS",
    momentum_list=["LF98TRUU_Index_OAS", "LUACTRUU_Index_OAS"],
    momentum_X_days=[5, 10, 15],
    momentum_Y_days=30
)

# Train model
model = _build_model(pipeline, model_name="XGBoost")

# Get predictions and metrics
predictions = model._return_preds()
mae, mse, rmse = model._return_mean_error_metrics()

# Analyze feature importance
model.predictive_power(forecast_range=30)
model._feature_importance(forecast_range=30)
model._feature_importance_over_time(forecast_range=30)
```

### Method 3: Using Data Source Abstraction

```python
from _data_sources import DataSourceFactory
from _config import get_config

# Load configuration
config = get_config()

# Create data source (Excel, CSV, or Bloomberg API)
source = DataSourceFactory.create(
    "excel",
    file_path="data/Economic_Data_2020_08_01.xlsx"
)
df = source.load_data()

# Use with preprocessing pipeline
preprocessing_config = config.get_preprocessing_config()
pipeline = _preprocess_xlsx(df, **preprocessing_config)
```

---

## Configuration

### YAML Configuration (config.yaml)

Create from template:
```bash
cp config.example.yaml config.yaml
```

**Example configuration:**
```yaml
# Data source
data_source:
  type: "excel"
  excel:
    file_path: "data/Economic_Data_2020_08_01.xlsx"

# Model settings
model:
  type: "XGBoost"
  estimators: 1000
  random_state: 42

# Feature engineering
features:
  target: "LF98TRUU_Index_OAS"
  momentum_columns:
    - "LF98TRUU_Index_OAS"
    - "LUACTRUU_Index_OAS"
  momentum_short_windows: [5, 10, 15]
  momentum_long_window: 30

# Analysis settings
analysis:
  importance_threshold: 0.05
  max_forecast_days: 30
```

See [config.example.yaml](config.example.yaml) for all options.

### Environment Variables (.env)

Create from template:
```bash
cp .env.example .env
```

**Example .env file:**
```bash
DATA_SOURCE_TYPE=excel
DATA_FILE_PATH=data/Economic_Data_2020_08_01.xlsx
MODEL_TYPE=XGBoost
TARGET_COLUMN=LF98TRUU_Index_OAS
NUMEXPR_MAX_THREADS=16
```

Environment variables override YAML settings.

---

## Data Sources

### Excel Files (Current Method)

**Requirements:**
- File format: `.xlsx` or `.xls`
- Must contain a `Dates` column with valid dates
- Numeric columns for economic indicators

**Bloomberg Export Instructions:**
1. Open Bloomberg Terminal
2. Use Excel plugin to export data
3. Save to `data/` folder
4. Ensure date column is named "Dates"

**Example file structure:**
```
Dates          | LF98TRUU_Index_OAS | LUACTRUU_Index_OAS | ...
2020-01-01     | 123.45             | 234.56             | ...
2020-01-02     | 125.67             | 236.78             | ...
```

### CSV Files

```python
from _data_sources import CSVDataSource

source = CSVDataSource(
    file_path="data/economic_data.csv",
    date_column="Date"
)
df = source.load_data()
```

### Bloomberg API (Template Provided)

**Setup:**
```bash
pip install blpapi
```

**Implementation:**
1. Edit `_data_sources.py`
2. Complete the `BloombergAPIDataSource.load_data()` method
3. Configure in `config.yaml`:

```yaml
data_source:
  type: "bloomberg"
  bloomberg:
    securities:
      - "LF98TRUU Index"
      - "LUACTRUU Index"
    fields:
      - "OAS"
      - "PX_LAST"
    start_date: "2020-01-01"
    end_date: "2020-12-31"
```

See [CHANGES.md](CHANGES.md) for detailed Bloomberg API integration guide.

---

## Model Details

### Supported Models

- **XGBoost** (Recommended) - Gradient boosted trees
- **CART** - Decision tree regressor
- **AdaBoost** - Adaptive boosting classifier
- **Logistic Regression** - Linear classification
- **Quadratic Regression** - Polynomial regression
- **K-Nearest Neighbors** - Instance-based learning

### Feature Engineering

**Momentum Features:**
Automatically creates rolling average features:

```
momentum = (short_term_avg - long_term_avg) / long_term_avg
```

- **Short-term windows**: 5, 10, 15 days (configurable)
- **Long-term baseline**: 30 days (configurable)
- **Formula**: `(X_day_avg - Y_day_avg) / Y_day_avg`

**Example:**
- Input column: `LF98TRUU_Index_OAS`
- Generated features:
  - `LF98TRUU_Index_OAS_5day_rolling_average`
  - `LF98TRUU_Index_OAS_10day_rolling_average`
  - `LF98TRUU_Index_OAS_15day_rolling_average`

### Evaluation Metrics

- **MAE** (Mean Absolute Error) - Average prediction error magnitude
- **MSE** (Mean Squared Error) - Squared error (penalizes large errors)
- **RMSE** (Root Mean Squared Error) - Error in original units
- **Predictive Power Score** - Feature predictive capability (0-1)
- **Feature Importance** - XGBoost tree-based importance scores
- **Cross-Validation** - 5-fold time series split validation

---

## Project Structure

```
BBG-Credit-Momentum/
├── webapp.py                  # Main Streamlit application
├── _preprocessing.py          # Data preprocessing pipeline
├── _models.py                 # Model training and analysis
├── _data_sources.py          # Data source abstraction layer
├── _config.py                # Configuration management
│
├── data/                     # Data files
│   └── Economic_Data_2020_08_01.xlsx
│
├── _img/                     # Generated visualizations
│   ├── arrow_logo.png
│   ├── predictive_power.png
│   ├── feats_importance.png
│   └── feats_importance_over_time.png
│
├── logs/                     # Application logs
│   ├── _main.log
│   ├── _model.log
│   ├── _preprocess.log
│   └── _data_sources.log
│
├── .streamlit/               # Streamlit configuration
│   └── config.toml
│
├── config.yaml               # Application configuration (create from example)
├── config.example.yaml       # Configuration template
├── .env                      # Environment variables (create from example)
├── .env.example              # Environment template
│
├── requirements.txt          # Python dependencies
├── README.md                 # This file
├── CHANGES.md                # Detailed changelog
└── LICENSE                   # MIT License
```

---

## Example Outputs

### Historical Data & Forecasts
![Historical Data](https://github.com/adrian-adduci/Bloomberg_Predictive_Modelling/blob/3c1415df764e103f68a542d6cbb434d1b9b71661/_img/example_forecast.PNG)

### Feature Importance Over Time
![Feature Importance Over Time](https://github.com/adrian-adduci/Bloomberg_Predictive_Modelling/blob/3c1415df764e103f68a542d6cbb434d1b9b71661/_img/feats_importance_over_time.png)

### Predictive Power Analysis
![Predictive Power](https://github.com/adrian-adduci/Bloomberg_Predictive_Modelling/blob/3c1415df764e103f68a542d6cbb434d1b9b71661/_img/predictive_power.png)


## Troubleshooting

### Common Issues

**"ModuleNotFoundError: No module named 'streamlit'"**
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**"FileNotFoundError: Excel file not found"**
```bash
# Solution: Check file path
# - Ensure file is in data/ folder
# - Use absolute path or correct relative path
# - Verify file extension (.xlsx or .xls)
```

**"ValueError: Target column not found"**
```bash
# Solution: Check column name
# - Open Excel file and verify column name
# - Ensure exact spelling (case-sensitive)
# - Check for leading/trailing spaces
```

**"Unable to open logs/_main.log"**
```bash
# Solution: Create logs directory
mkdir logs
```

**Streamlit shows "Please select a file"**
```bash
# Solution: Upload file or check file_buffer
# - Use file uploader in sidebar
# - Verify file is uploaded successfully
```

### Performance Tips

- **Large datasets**: Increase `NUMEXPR_MAX_THREADS` in `.env`
- **Memory issues**: Reduce `momentum_X_days` windows
- **Slow training**: Reduce `estimators` in XGBoost config
- **Disk space**: Clear `_img/` folder periodically

---

## Testing

### Manual Testing

```bash
# Test with example data
streamlit run webapp.py
# Upload: data/Economic_Data_2020_08_01.xlsx
# Target: LF98TRUU_Index_OAS
# Momentum: LF98TRUU_Index_OAS,LUACTRUU_Index_OAS
```

### Python Testing

```python
# Test preprocessing
from _preprocessing import _preprocess_xlsx

pipeline = _preprocess_xlsx(
    "data/Economic_Data_2020_08_01.xlsx",
    "LF98TRUU_Index_OAS"
)
print(f"Loaded {len(pipeline._return_dataframe())} rows")

# Test model training
from _models import _build_model

model = _build_model(pipeline)
mae, mse, rmse = model._return_mean_error_metrics()
print(f"RMSE: {rmse:.4f}")
```

### Unit Tests (Recommended)

Coming soon! Contributions welcome.

---

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/BBG-Credit-Momentum.git
cd BBG-Credit-Momentum

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Make changes and test
streamlit run webapp.py

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Author

**Adrian Adduci**
- Email: FAA2160@columbia.edu
- GitHub: [@adrian-adduci](https://github.com/adrian-adduci)

---

## Additional Resources

- [Streamlit Documentation](https://docs.streamlit.io/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [pandas Documentation](https://pandas.pydata.org/docs/)
- [scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Bloomberg API Documentation](https://www.bloomberg.com/professional/support/api-library/)

